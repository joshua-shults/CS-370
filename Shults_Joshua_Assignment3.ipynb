{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential \n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten \n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D \n",
    "from keras.optimizers import SGD, Adam, RMSprop \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# CIFAR_10 is a set of 60k images 32x32 pixels 3 channels\n",
    "\n",
    "IMG_ROWS = 32 \n",
    "IMG_COLS = 32 \n",
    "IMG_CHANNELS = 3\n",
    "#constant \n",
    "BATCH_SIZE = 128 \n",
    "NB_EPOCH = 20 \n",
    "NB_CLASSES = 10 \n",
    "VERBOSE = 1 \n",
    "VALIDATION_SPLIT = 0.2 \n",
    "OPTIM = RMSprop() \n",
    "\n",
    "#load dataset \n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data() \n",
    "print('X_train shape:', X_train.shape) \n",
    "print(X_train.shape[0], 'train samples') \n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "\n",
    "# convert to categorical \n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES) \n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES) \n",
    "# float and normalization \n",
    "X_train = X_train.astype('float32') \n",
    "X_test = X_test.astype('float32') \n",
    "X_train /= 255 \n",
    "X_test /= 255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               4194816   \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 4,200,842\n",
      "Trainable params: 4,200,842\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 66s 2ms/step - loss: 1.7132 - accuracy: 0.3982 - val_loss: 1.4260 - val_accuracy: 0.4855\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 61s 2ms/step - loss: 1.3579 - accuracy: 0.5171 - val_loss: 1.2412 - val_accuracy: 0.5602\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 57s 1ms/step - loss: 1.2355 - accuracy: 0.5633 - val_loss: 1.1809 - val_accuracy: 0.6005\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 68s 2ms/step - loss: 1.1536 - accuracy: 0.5945 - val_loss: 1.1816 - val_accuracy: 0.5875\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 64s 2ms/step - loss: 1.0842 - accuracy: 0.6172 - val_loss: 1.1445 - val_accuracy: 0.6128\n",
      "Epoch 6/20\n",
      "40000/40000 [==============================] - 54s 1ms/step - loss: 1.0343 - accuracy: 0.6380 - val_loss: 1.4367 - val_accuracy: 0.5215\n",
      "Epoch 7/20\n",
      "40000/40000 [==============================] - 57s 1ms/step - loss: 0.9837 - accuracy: 0.6543 - val_loss: 1.0325 - val_accuracy: 0.6457\n",
      "Epoch 8/20\n",
      "40000/40000 [==============================] - 60s 1ms/step - loss: 0.9392 - accuracy: 0.6727 - val_loss: 1.0311 - val_accuracy: 0.6488\n",
      "Epoch 9/20\n",
      "40000/40000 [==============================] - 53s 1ms/step - loss: 0.8978 - accuracy: 0.6855 - val_loss: 0.9917 - val_accuracy: 0.6632\n",
      "Epoch 10/20\n",
      "40000/40000 [==============================] - 54s 1ms/step - loss: 0.8612 - accuracy: 0.7001 - val_loss: 1.0643 - val_accuracy: 0.6492\n",
      "Epoch 11/20\n",
      "40000/40000 [==============================] - 51s 1ms/step - loss: 0.8275 - accuracy: 0.7128 - val_loss: 1.0000 - val_accuracy: 0.6608\n",
      "Epoch 12/20\n",
      "40000/40000 [==============================] - 53s 1ms/step - loss: 0.7905 - accuracy: 0.7251 - val_loss: 1.1011 - val_accuracy: 0.6368\n",
      "Epoch 13/20\n",
      "40000/40000 [==============================] - 54s 1ms/step - loss: 0.7668 - accuracy: 0.7355 - val_loss: 0.9682 - val_accuracy: 0.6820\n",
      "Epoch 14/20\n",
      "40000/40000 [==============================] - 52s 1ms/step - loss: 0.7410 - accuracy: 0.7428 - val_loss: 1.0008 - val_accuracy: 0.6715\n",
      "Epoch 15/20\n",
      "40000/40000 [==============================] - 54s 1ms/step - loss: 0.7139 - accuracy: 0.7548 - val_loss: 0.9806 - val_accuracy: 0.6815\n",
      "Epoch 16/20\n",
      "40000/40000 [==============================] - 53s 1ms/step - loss: 0.6863 - accuracy: 0.7640 - val_loss: 0.9701 - val_accuracy: 0.6877\n",
      "Epoch 17/20\n",
      "40000/40000 [==============================] - 52s 1ms/step - loss: 0.6635 - accuracy: 0.7679 - val_loss: 1.0440 - val_accuracy: 0.6673\n",
      "Epoch 18/20\n",
      "40000/40000 [==============================] - 51s 1ms/step - loss: 0.6405 - accuracy: 0.7761 - val_loss: 1.0033 - val_accuracy: 0.6874\n",
      "Epoch 19/20\n",
      "40000/40000 [==============================] - 48s 1ms/step - loss: 0.6220 - accuracy: 0.7847 - val_loss: 1.0302 - val_accuracy: 0.6785\n",
      "Epoch 20/20\n",
      "40000/40000 [==============================] - 51s 1ms/step - loss: 0.6014 - accuracy: 0.7920 - val_loss: 1.0422 - val_accuracy: 0.6773\n",
      "10000/10000 [==============================] - 5s 493us/step\n",
      "Test score: 1.0380128435134888\n",
      "Test accuracy: 0.6672000288963318\n"
     ]
    }
   ],
   "source": [
    "# network \n",
    "model = Sequential() \n",
    "model.add(Conv2D(32, (3, 3), padding='same', \n",
    "input_shape=(IMG_ROWS, IMG_COLS, IMG_CHANNELS))) \n",
    "model.add(Activation('relu')) \n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) \n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Flatten()) \n",
    "model.add(Dense(512)) \n",
    "model.add(Activation('relu')) \n",
    "model.add(Dropout(0.5)) \n",
    "model.add(Dense(NB_CLASSES)) \n",
    "model.add(Activation('softmax')) \n",
    "model.summary()\n",
    "\n",
    "# train \n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIM, \n",
    "metrics=['accuracy']) \n",
    "model.fit(X_train, Y_train, batch_size=BATCH_SIZE, \n",
    "epochs=NB_EPOCH, validation_split=VALIDATION_SPLIT, \n",
    "verbose=VERBOSE) \n",
    "score = model.evaluate(X_test, Y_test, \n",
    "batch_size=BATCH_SIZE, verbose=VERBOSE) \n",
    "print(\"Test score:\", score[0]) \n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "\n",
    "#save model \n",
    "model_json = model.to_json() \n",
    "open('cifar10_architecture.json', 'w').write(model_json) \n",
    "#And the weights learned by our deep network on the training \n",
    "model.save_weights('cifar10_weights.h5', overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3))`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "model = Sequential() \n",
    "model.add(Conv2D(32, (3, 3), padding='same', \n",
    "input_shape=(IMG_ROWS, IMG_COLS, IMG_CHANNELS))) \n",
    "model.add(Activation('relu')) \n",
    "model.add(Conv2D(32, (3, 3), padding='same')) \n",
    "model.add(Activation('relu')) \n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) \n",
    "model.add(Dropout(0.25)) \n",
    "model.add(Conv2D(64, (3, 3), padding='same')) \n",
    "model.add(Activation('relu')) \n",
    "model.add(Conv2D(64, 3, 3)) \n",
    "model.add(Activation('relu')) \n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) \n",
    "model.add(Dropout(0.25)) \n",
    "model.add(Flatten()) \n",
    "model.add(Dense(512))\n",
    "\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting training set images...\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.datasets import cifar10\n",
    "import numpy as np\n",
    "NUM_TO_AUGMENT = 5\n",
    "\n",
    "#load dataset\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "#augmenting\n",
    "print(\"Augmenting training set images...\")\n",
    "\n",
    "datagen = ImageDataGenerator( \n",
    "    rotation_range=40, \n",
    "    width_shift_range=0.2, \n",
    "    height_shift_range=0.2, \n",
    "    zoom_range=0.2, \n",
    "    horizontal_flip=True, \n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "xtas, ytas = [], [] \n",
    "for i in range(X_train.shape[0]): \n",
    "    num_aug = 0 \n",
    "    x = X_train[i]  # (32, 32, 3) \n",
    "    x = x.reshape((1,) + x.shape)  # (1, 32, 32, 3) \n",
    "    for x_aug in datagen.flow(x, batch_size=1, \n",
    "                               save_to_dir='preview', save_prefix='cifar', save_format='jpeg'): \n",
    "        if num_aug >= NUM_TO_AUGMENT: \n",
    "            break \n",
    "        xtas.append(x_aug[0]) \n",
    "        num_aug += 1\n",
    "\n",
    "#fit the dataset \n",
    "datagen.fit(X_train) \n",
    "\n",
    "# train \n",
    "history = model.fit_generator(datagen.flow(X_train, Y_train, \n",
    "                                           batch_size=BATCH_SIZE), \n",
    "                              samples_per_epoch=X_train.shape[0], \n",
    "                              epochs=NB_EPOCH, verbose=VERBOSE) \n",
    "\n",
    "score = model.evaluate(X_test, Y_test, \n",
    "                       batch_size=BATCH_SIZE, verbose=VERBOSE) \n",
    "\n",
    "print(\"Test score:\", score[0]) \n",
    "print('Test accuracy:', score[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markdown Cell\n",
    "\n",
    "\n",
    "This algorithm is trained on the CIFAR-10 dataset, which the textbook states is capable of dinstinguishing between images of animals and vehicles. I believe that this is harmless, and that training a model on identifying things such as this does not violate any privacy or cause issues otherwise. However, these networks are able to be trained on sensitive and personal data, for instance they can be trained for facial recognition.\n",
    "\n",
    "First there is the issue of privacy violations. In January 2020, Detroit Police wrongfully arrested a man named Robert Williams due to a false positive in a facial recognition system. He was arrested in front of his entire family, and spent roughly 30 hours in jail. Robert Williams is a black male, and facial recognition systems are reported to be much less accurate on people of races other than white. The New York Times reported that one popular facial recognition data set is 75% male, and 80% white. (New York Times, 2018) In testing the accuracy of facial recognition software on diverse faces, Joy Buolamwini found that Microsoft had an error rate for darker-skinned women at 21 percent, and IBM had a error rate of about 35%. Also in her findings, she found that they all had error rates below 1% for 'light-skinned males.' (Buolamwini, 2018) From the above findings, it can be concluded that there is more at stake than privacy issues like your face being used in a law enforcement database, models can have bias and act discriminatorily. This bias can lead to innocent people like Robert Williams, who was fortunately released, being arrested and subject to further discrimination and public humiliation. A wrongful arrest also could have the even worse result of injury or loss of life, if for instance someone is falsely found as a suspect in an incorrect facial-recognition system of a violent crime. In conclusion, image classification algorithms such as the one used for this assignment should be viewed for their strengths and weaknesses. If you are training a model on human faces, detecting and removing bias should be on the forefront of the entire development.\n",
    "\n",
    "REFERENCES:\n",
    "\n",
    "Lohr, Steve. (February 9, 2018). Facial Recognition Is Accurate, if You're a White Guy. NY TIMES. https://www.nytimes.com/2018/02/09/technology/facial-recognition-race-artificial-intelligence.html\n",
    "\n",
    "Buolamwini, Joy. Gebru, Timmnit. (2018). Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification. MIT. https://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf\n",
    "\n",
    "ACLU. (January 29, 2024). Williams v. City of Detroit. ACLU. https://www.aclu.org/cases/williams-v-city-of-detroit-face-recognition-false-arrest\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
